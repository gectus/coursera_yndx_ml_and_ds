{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Roberta Sentiment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2e2a1089772b4e34b7762026e4beccf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d805cb1cd073403388c36700c4c5beb7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d3d04c0268614762bd2241ffa1c77a17",
              "IPY_MODEL_d614a2de2bc344aa9d9cde48a3ab9480"
            ]
          }
        },
        "d805cb1cd073403388c36700c4c5beb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d3d04c0268614762bd2241ffa1c77a17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_62783489f75540e1ac8380a0951f003e",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 898823,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 898823,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b1538b4db3384c0e942c0df234248914"
          }
        },
        "d614a2de2bc344aa9d9cde48a3ab9480": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3a056ba348534a008e42044defff8f29",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 899k/899k [00:01&lt;00:00, 760kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_986daf4f21914b9ca84785a4a5254654"
          }
        },
        "62783489f75540e1ac8380a0951f003e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b1538b4db3384c0e942c0df234248914": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3a056ba348534a008e42044defff8f29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "986daf4f21914b9ca84785a4a5254654": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0963d102f2c94081892c43e34198a4da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_640d9fc208e24e798b1bac5aad8a7bd5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1d4eccb27e904d3f9c49e856e5b018b6",
              "IPY_MODEL_580d90a7cfc546449f2e0e5c32b5468d"
            ]
          }
        },
        "640d9fc208e24e798b1bac5aad8a7bd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1d4eccb27e904d3f9c49e856e5b018b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6e5f0790ed04460f88dcd3a76a95a7e9",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a80a881057944281b5e43824a048af78"
          }
        },
        "580d90a7cfc546449f2e0e5c32b5468d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b6f37fd779dd42dbbe3a31074e9e0a84",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:00&lt;00:00, 1.24MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5978dedcac0b4b7b80e3a9adbf1dac50"
          }
        },
        "6e5f0790ed04460f88dcd3a76a95a7e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a80a881057944281b5e43824a048af78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b6f37fd779dd42dbbe3a31074e9e0a84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5978dedcac0b4b7b80e3a9adbf1dac50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2e9250de84d9499888d2fdd71c0689fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_48e4c646b2744792a3671abdf3d4f87b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b1dd654a858848339fd3645473130c55",
              "IPY_MODEL_84764d9badb64b18a76fe90e1b345cc8"
            ]
          }
        },
        "48e4c646b2744792a3671abdf3d4f87b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b1dd654a858848339fd3645473130c55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bb0697eab67b4040a2eb5466e50b6a4c",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 481,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 481,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bd6087ff0d7c4106949e96b52fac484f"
          }
        },
        "84764d9badb64b18a76fe90e1b345cc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_178ea1a7b4244aedb19de094d647c581",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 481/481 [00:00&lt;00:00, 1.46kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ef889e5c3d144ad09f5e12bc50dee5bc"
          }
        },
        "bb0697eab67b4040a2eb5466e50b6a4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bd6087ff0d7c4106949e96b52fac484f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "178ea1a7b4244aedb19de094d647c581": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ef889e5c3d144ad09f5e12bc50dee5bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "620c1a42ce7e44c4afef538f8e52d7c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8334f402ed204748928392aeaa0605d4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ef1cb1a0789b46c383245497d6cddf54",
              "IPY_MODEL_a58e4b2820e346fdb2f918d02d33692c"
            ]
          }
        },
        "8334f402ed204748928392aeaa0605d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ef1cb1a0789b46c383245497d6cddf54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ff6fb2dcbfcb4a399978f4cd464eaba5",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 501200538,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 501200538,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c4061bdbca4f4f66948ee625b186a32c"
          }
        },
        "a58e4b2820e346fdb2f918d02d33692c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f025cb9bee684fd88382d4aeff2b0227",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 501M/501M [00:51&lt;00:00, 9.65MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ea5f6ee1e21b4b2ba8d780bff052b309"
          }
        },
        "ff6fb2dcbfcb4a399978f4cd464eaba5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c4061bdbca4f4f66948ee625b186a32c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f025cb9bee684fd88382d4aeff2b0227": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ea5f6ee1e21b4b2ba8d780bff052b309": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldJddPevW1fh"
      },
      "source": [
        "# <b>Соревнование по сентимент-анализу отзывов</b>\n",
        "[Kaggle Sentiment Analysis](https://www.kaggle.com/c/simplesentiment)\n",
        "\n",
        "Due to the selected approach we will use PyTorch, Google Colab + GPU in our solution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjiDHVxiYiwm"
      },
      "source": [
        "There are various model architectures we can implement to do a sentiment analysis of sentences apart from baseline solution suggested in the class. For example:\n",
        "\n",
        "- Logistic regression by converting words into vectors by using bag of words\n",
        "- Random Forests build using nlp features\n",
        "- Converting words into vectors using word embeddings and using RNN's\n",
        "- Converting words into vectors using word embeddings and using CNN's\n",
        "- Combing RNN's and CNN's\n",
        "\n",
        "There are numerous number of NN architectures that can be used in sentiment analysys for our purposes. For instance, GPT-3, Elmo or BERT.\n",
        "\n",
        "#### <b>BERT (Bidirectional Encoder Representations from Transformers)</b>\n",
        "\n",
        "In 2018, Google AI released a model called BERT. If you don't know what BERT is, please go through the following links:\n",
        "\n",
        "- [Paper: \"BERT: Pre-training of Deep Bidirectional Transformers forLanguage Understanding\"](https://arxiv.org/pdf/1810.04805.pdf)\n",
        "\n",
        "BERT obtains new state-of-the-art results on eleven natural language processing tasks.\n",
        "\n",
        "#### <b>RoBERTa: A Robustly Optimized BERT Pretraining Approach</b>\n",
        "\n",
        "- [Paper](https://arxiv.org/abs/1907.11692)\n",
        "- [Facebook Blog](https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/)\n",
        "\n",
        "\n",
        " We will use the state-of-the-art model architecture <b>ROBERTA</b> implemented in PyTorch and created by Facebook AI team which proposed an 'optimized' and 'robust' version of BERT in 2019.\n",
        "\n",
        "It essentially includes fine-tuning the original BERT model along with data and inputs manipulation.\n",
        "\n",
        "![image.png](https://miro.medium.com/max/700/1*bSUO_Qib4te1xQmBlQjWaw.png)\n",
        "\n",
        "#### <b>Data</b>\n",
        "\n",
        "It has been observed that training BERT on larger datasets, greatly improves its performance. So RoBERTa is trained on a vast dataset that goes over 160GB of uncompressed text. This dataset is composed of the following corpora:\n",
        "\n",
        "1. BookCorpus + English Wikipedia (16GB): This is the data on which BERT is trained.\n",
        "2. CC-News (76GB): The authors have collected this data from the English portion of the CommonCrawl News Data. It contains 63M English news articles crawled between September 2016 and February 2019.\n",
        "3. OpenWebText (38GB): Open Source recreation of the WebText dataset used to train OpenAI GPT.\n",
        "4. Stories (31GB): A subset of CommonCrawl data filtered to match the story-like style of Winograd schemas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqN2KWMAmZcb"
      },
      "source": [
        "### <b>1. Install and import libraries</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0vs3F3sYas3"
      },
      "source": [
        "First, let's install and import necessary libraries like [nltk](https://www.nltk.org/) to perform basic text preprocessing and \n",
        "state-of-the-art Natural Language Processing for PyTorch  [transformers](https://github.com/huggingface/transformers) library by <img src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f917.png\" alt=\"huggingface\" title=\"huggingface\" width=\"15\">.\n",
        "\n",
        "<img src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f917.png\" width=\"15\" title=\"huggingface\"> Huggingface Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation, etc in 100+ languages. Its aim is to make cutting-edge NLP easier to use for everyone."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZg3fziXRufB",
        "outputId": "93b38006-c9cb-4441-eee0-94b10c244b64"
      },
      "source": [
        "!pip3 install transformers nltk"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/84/7bc03215279f603125d844bf81c3fb3f2d50fe8e511546eb4897e4be2067/transformers-4.0.0-py3-none-any.whl (1.4MB)\n",
            "\r\u001b[K     |▎                               | 10kB 22.3MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 17.7MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 14.7MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 13.2MB/s eta 0:00:01\r\u001b[K     |█▏                              | 51kB 10.5MB/s eta 0:00:01\r\u001b[K     |█▌                              | 61kB 9.2MB/s eta 0:00:01\r\u001b[K     |█▊                              | 71kB 10.3MB/s eta 0:00:01\r\u001b[K     |██                              | 81kB 11.4MB/s eta 0:00:01\r\u001b[K     |██▏                             | 92kB 10.5MB/s eta 0:00:01\r\u001b[K     |██▍                             | 102kB 10.0MB/s eta 0:00:01\r\u001b[K     |██▋                             | 112kB 10.0MB/s eta 0:00:01\r\u001b[K     |███                             | 122kB 10.0MB/s eta 0:00:01\r\u001b[K     |███▏                            | 133kB 10.0MB/s eta 0:00:01\r\u001b[K     |███▍                            | 143kB 10.0MB/s eta 0:00:01\r\u001b[K     |███▋                            | 153kB 10.0MB/s eta 0:00:01\r\u001b[K     |███▉                            | 163kB 10.0MB/s eta 0:00:01\r\u001b[K     |████▏                           | 174kB 10.0MB/s eta 0:00:01\r\u001b[K     |████▍                           | 184kB 10.0MB/s eta 0:00:01\r\u001b[K     |████▋                           | 194kB 10.0MB/s eta 0:00:01\r\u001b[K     |████▉                           | 204kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████                           | 215kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 225kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 235kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 245kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 256kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 266kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 276kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 286kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 296kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 307kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 317kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 327kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 337kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 348kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 358kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 368kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 378kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 389kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 399kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 409kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 419kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 430kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 440kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 450kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 460kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 471kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 481kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 491kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 501kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 512kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 522kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 532kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 542kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 552kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 563kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 573kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 583kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 593kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 604kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 614kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 624kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 634kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 645kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 655kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 665kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 675kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 686kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 696kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 706kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 716kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 727kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 737kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 747kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 757kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 768kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 778kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 788kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 798kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 808kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 819kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 829kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 839kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 849kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 860kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 870kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 880kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 890kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 901kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 911kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 921kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 931kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 942kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 952kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 962kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 972kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 983kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 993kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.0MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.0MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.0MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.0MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.0MB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.1MB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.1MB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.1MB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.1MB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.1MB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.1MB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.1MB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.1MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.1MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.1MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.2MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.2MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.2MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.2MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.2MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.2MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.2MB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.2MB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.2MB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.2MB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.3MB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.3MB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.3MB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.3MB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.3MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.3MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.3MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.3MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.3MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.4MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.4MB 10.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 31.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 41.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=c482a3cf6e099d3b81eafc9fe0b8d3a5907156de36601ea8cc401fc5ac755973\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAORmzRzQBqv",
        "outputId": "e9c99e83-27e9-4cba-9944-4e33c8717646"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import glob\n",
        "import logging\n",
        "import os\n",
        "import csv\n",
        "import logging\n",
        "import sys\n",
        "import random\n",
        "import json\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from sklearn.metrics import matthews_corrcoef, f1_score\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig, RobertaModel, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "csv.field_size_limit(2147483647)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "131072"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28oBYS7LbSZh"
      },
      "source": [
        "Authorize Google Drive in Colab and mount drive in order to use our datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxrEAdY1PhwZ",
        "outputId": "d130417d-5b6f-49cb-f977-2f2565c74c4a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YztHjegkbmCm"
      },
      "source": [
        "We'll use a GPU in our training process, so let's check which GPU we are using in a kernel by typing <code>!nvidia-smi<code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEbhHO327R2K",
        "outputId": "c2a78701-4095-481b-de9b-dcc3d5c2ca2b"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Dec  2 00:07:40 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8    11W /  70W |     10MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPjihdZCb0Ii"
      },
      "source": [
        "Set device to CUDA if GPU is avaliable, else set CPU to perform calculations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CB9C-Dll7TR7",
        "outputId": "587d0d08-ec22-4e88-fb21-ac6f88c88cc4"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtjqDQ24cAH6"
      },
      "source": [
        "Initialize <code>preprocess</code> function to perform a basic clean of our dataset, remove HTML tags, numbers, punctuation and put sentences to lowercase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGq1VBC2mktN"
      },
      "source": [
        "### <b>2. Load data</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnkmU9WoRJ-V"
      },
      "source": [
        "def preprocess(text):\n",
        "  \n",
        "  new_text = re.sub('<.*?>', '', text) # remove html tags\n",
        "  new_text = re.sub(r'[^\\w\\s]', '', new_text) # remove punctiation\n",
        "  new_text = re.sub(fr'\\b\\d+\\b', '', new_text ) # remove numbers (not if they're in words)\n",
        "  \n",
        "  new_text = new_text.lower() # lowercase sentences      \n",
        "\n",
        "  return new_text"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9z7b5ZAcc7I"
      },
      "source": [
        "Load our dataset for ROBERTA fine-tuning on 2000 examples. \n",
        "\n",
        "Also let's see if our classes are imbalanced (\"0\" and \"1\") by visualizing classes distribution using Matplotlib and pie chart"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "xPqMcJh2QD1z",
        "outputId": "ee83e914-8e0a-4c5e-eb42-161252b0e008"
      },
      "source": [
        "train_df = pd.read_csv('/content/gdrive/My Drive/products_sentiment_train.tsv', sep='\\t', header=None)\n",
        "train_df = train_df.reset_index()\n",
        "train_df.columns = ['id', 'text', 'label']\n",
        "train_df['label'] = train_df['label'].astype(str)\n",
        "\n",
        "train_df"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2 . take around 10,000 640x480 pictures .</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>i downloaded a trial version of computer assoc...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>the wrt54g plus the hga7t is a perfect solutio...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>i dont especially like how music files are uns...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>i was using the cheapie pail ... and it worked...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>1995</td>\n",
              "      <td>speaker phone quality is good , and poping in ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>1996</td>\n",
              "      <td>the \" movies \" last about 5 seconds .</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>1997</td>\n",
              "      <td>overall i like it .</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>1998</td>\n",
              "      <td>i began taking pics as soon as i got this came...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>1999</td>\n",
              "      <td>even after reading some of the instructions , ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id                                               text label\n",
              "0        0          2 . take around 10,000 640x480 pictures .     1\n",
              "1        1  i downloaded a trial version of computer assoc...     1\n",
              "2        2  the wrt54g plus the hga7t is a perfect solutio...     1\n",
              "3        3  i dont especially like how music files are uns...     0\n",
              "4        4  i was using the cheapie pail ... and it worked...     1\n",
              "...    ...                                                ...   ...\n",
              "1995  1995  speaker phone quality is good , and poping in ...     1\n",
              "1996  1996              the \" movies \" last about 5 seconds .     0\n",
              "1997  1997                               overall i like it .      1\n",
              "1998  1998  i began taking pics as soon as i got this came...     1\n",
              "1999  1999  even after reading some of the instructions , ...     0\n",
              "\n",
              "[2000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "hXT4z2H57ztk",
        "outputId": "b55da7b4-576f-4628-af5e-e6f3130eb09a"
      },
      "source": [
        "train_df.label.value_counts().plot(kind='pie', label='Sentiment',\n",
        "                                       autopct='%.3f%%', pctdistance=1.4, labeldistance=1.05)\n",
        "plt.legend()\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZHYiMzWdQ57"
      },
      "source": [
        "Classes are imbalanced relatively about 2 to 1, so it is neccessary to balance our data by either:\n",
        "\n",
        "1. Using NLP Data Augmentation\n",
        "2. Oversampling minority class\n",
        "3. Undersampling majotiry class\n",
        "4. Use Weighted Loss function, e.g. Weighted BinaryCrossEntropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "E6Fz6X4T8U05",
        "outputId": "d142172a-92c0-4b41-8241-18b0653be228"
      },
      "source": [
        "train_df['text'] = train_df['text'].apply(lambda x: preprocess(x)).astype(str)\n",
        "train_df"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>take around  640x480 pictures</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>i downloaded a trial version of computer assoc...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>the wrt54g plus the hga7t is a perfect solutio...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>i dont especially like how music files are uns...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>i was using the cheapie pail  and it worked ok...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>1995</td>\n",
              "      <td>speaker phone quality is good  and poping in a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>1996</td>\n",
              "      <td>the  movies  last about  seconds</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>1997</td>\n",
              "      <td>overall i like it</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>1998</td>\n",
              "      <td>i began taking pics as soon as i got this came...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>1999</td>\n",
              "      <td>even after reading some of the instructions  i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id                                               text label\n",
              "0        0                     take around  640x480 pictures      1\n",
              "1        1  i downloaded a trial version of computer assoc...     1\n",
              "2        2  the wrt54g plus the hga7t is a perfect solutio...     1\n",
              "3        3  i dont especially like how music files are uns...     0\n",
              "4        4  i was using the cheapie pail  and it worked ok...     1\n",
              "...    ...                                                ...   ...\n",
              "1995  1995  speaker phone quality is good  and poping in a...     1\n",
              "1996  1996                  the  movies  last about  seconds      0\n",
              "1997  1997                                overall i like it       1\n",
              "1998  1998  i began taking pics as soon as i got this came...     1\n",
              "1999  1999  even after reading some of the instructions  i...     0\n",
              "\n",
              "[2000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDPSqt3rmt9U"
      },
      "source": [
        "### <b>3. Roberta model </b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8frEictWR-R1"
      },
      "source": [
        "# Class representing a single example object\n",
        "\n",
        "class InputExample(object):\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "        Args:\n",
        "            guid: Unique id for the example.\n",
        "            text_a: string. The untokenized text of the first sequence. For single\n",
        "            sequence tasks, only this sequence must be specified.\n",
        "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "            Only must be specified for sequence pair tasks.\n",
        "            label: (Optional) string. The label of the example. This should be\n",
        "            specified for train and dev examples, but not for test examples.\n",
        "        \"\"\"\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label\n",
        "\n",
        "\n",
        "# Class representing input features of a single example object\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_id = label_id"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l_uR1rVKF0O"
      },
      "source": [
        "# Data PreProcessor class, which creates train and test examples objects\n",
        "\n",
        "class DataPreProcessor():\n",
        "    \n",
        "    def get_labels(self):\n",
        "        \"\"\"Returns all the types of labels present in the data.\"\"\"\n",
        "        \n",
        "        return [\"0\", \"1\"]\n",
        "    \n",
        "    def get_train_examples(self, data):\n",
        "        \"\"\"Creates the examples for the given data.\"\"\"\n",
        "        examples = []\n",
        "        \n",
        "        for (i, line) in enumerate(data.values.tolist()):\n",
        "            \n",
        "            guid = line[0]\n",
        "            text_a = line[1]\n",
        "            label = line[-1]\n",
        "            \n",
        "            examples.append(InputExample(guid=guid, text_a=text_a, label=label))\n",
        "        \n",
        "        return examples\n",
        "\n",
        "    def get_dev_examples(self, data):\n",
        "        \"\"\"Creates the examples for the given data.\"\"\"\n",
        "        examples = []\n",
        "        \n",
        "        for (i, line) in enumerate(data.values.tolist()):\n",
        "            \n",
        "            guid = line[0]\n",
        "            text_a = line[1]\n",
        "            label = line[-1]\n",
        "            \n",
        "            examples.append(InputExample(guid=guid, text_a=text_a, label=label))\n",
        "        \n",
        "        return examples"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDs9nyOQd7Ep"
      },
      "source": [
        "Description of basic parameters and hyperparameters of our fine-tuned ROBERTA model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YY6L0iVCI4g"
      },
      "source": [
        "output_mode = 'classification' # or regression\n",
        "roberta_model = 'roberta-base' # model to use: bert, roberta, xl, xlm-roberta, etc\n",
        "\n",
        "train_batch_size = 32 # batch size for training(8, 16, 64, 128, ...), depends on GPU and memory management, less GPU memory -> less batch size\n",
        "gradient_accumulation_steps = 1 # batch at each step will be divided by this integer and gradient will be accumulated over\n",
        "max_grad_norm = 1.0 # parameter for clipping gradients, otherwise their value might explode\n",
        "num_train_epochs = 3.0 # epochs\n",
        "\n",
        "learning_rate = 1e-5 # adam optimizer learning rate\n",
        "warmup_proportion = 0.1 # learning rate value linear warmup proportion\n",
        "num_training_steps = 1000\n",
        "num_warmup_steps = 100\n",
        "max_seq_length = 256 # maximum sequence length of an input sentence, max value is: 512\n",
        "\n",
        "eval_batch_size = 8 # batch size for evaluation\n",
        "\n",
        "cache_dir = os.path.join(\"\", \"pytorch_pretrained_models\")\n",
        "output_dir = os.path.join(\"\", \"out_dir\") "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21DVVDsYDDDW"
      },
      "source": [
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSkSbkd6eFcL"
      },
      "source": [
        "Initialize DataProcessor for our training set and download ROBERTA tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205,
          "referenced_widgets": [
            "2e2a1089772b4e34b7762026e4beccf0",
            "d805cb1cd073403388c36700c4c5beb7",
            "d3d04c0268614762bd2241ffa1c77a17",
            "d614a2de2bc344aa9d9cde48a3ab9480",
            "62783489f75540e1ac8380a0951f003e",
            "b1538b4db3384c0e942c0df234248914",
            "3a056ba348534a008e42044defff8f29",
            "986daf4f21914b9ca84785a4a5254654",
            "0963d102f2c94081892c43e34198a4da",
            "640d9fc208e24e798b1bac5aad8a7bd5",
            "1d4eccb27e904d3f9c49e856e5b018b6",
            "580d90a7cfc546449f2e0e5c32b5468d",
            "6e5f0790ed04460f88dcd3a76a95a7e9",
            "a80a881057944281b5e43824a048af78",
            "b6f37fd779dd42dbbe3a31074e9e0a84",
            "5978dedcac0b4b7b80e3a9adbf1dac50"
          ]
        },
        "id": "uZNtMw_vK_m1",
        "outputId": "a471c199-4020-4a00-cdf0-3295804e43be"
      },
      "source": [
        "processor = DataPreProcessor() # data preprocessor\n",
        "\n",
        "label_list = processor.get_labels() # label list, e.g [0, 1, C], where C is number of classes\n",
        "num_labels = len(label_list)\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(roberta_model) # roberta tokenizer"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:filelock:Lock 139679408878200 acquired on /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2e2a1089772b4e34b7762026e4beccf0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898823.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:filelock:Lock 139679408878200 released on /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:filelock:Lock 139679428625016 acquired on /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0963d102f2c94081892c43e34198a4da",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:filelock:Lock 139679428625016 released on /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZz2EaK2xxlX"
      },
      "source": [
        "Use Regularization\n",
        "\n",
        "Paper: [Scheduled DropHead: A Regularization Method for Transformer Models](https://arxiv.org/pdf/2004.13342.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLNHFE85tdGr"
      },
      "source": [
        "VALID_CLS = (RobertaModel)\n",
        "\n",
        "\n",
        "def _drophead_hook(module, input, output):\n",
        "    \"\"\"\n",
        "    Pytorch forward hook for transformers.modeling_bert.BertSelfAttention layer\n",
        "    \"\"\"\n",
        "    if (not module.training) or (module.p_drophead==0):\n",
        "        return output\n",
        "\n",
        "    orig_shape = output[0].shape\n",
        "    dist = torch.distributions.Bernoulli(torch.tensor([1-module.p_drophead]))\n",
        "    mask = dist.sample((orig_shape[0], module.num_attention_heads))\n",
        "    mask = mask.to(output[0].device).unsqueeze(-1)\n",
        "    count_ones = mask.sum(dim=1).unsqueeze(-1)  # calc num of active heads\n",
        "\n",
        "    self_att_out = module.transpose_for_scores(output[0])\n",
        "    self_att_out = self_att_out * mask * module.num_attention_heads / count_ones\n",
        "    self_att_out = self_att_out.permute(0, 2, 1, 3).view(*orig_shape)\n",
        "    return (self_att_out,) + output[1:]\n",
        "\n",
        "\n",
        "def valid_type(obj):\n",
        "    return isinstance(obj, VALID_CLS)\n",
        "\n",
        "\n",
        "def get_base_model(model):\n",
        "    \"\"\"\n",
        "    Check model type. If correct then return the model itself.\n",
        "    If not correct then try to find in attributes and return correct type\n",
        "    attribute if found\n",
        "    \"\"\"\n",
        "    if not valid_type(model):\n",
        "        attrs = [name for name in dir(model) if valid_type(getattr(model, name))]\n",
        "        if len(attrs) == 0:\n",
        "            raise ValueError(\"Please provide valid model\")\n",
        "        model =  getattr(model, attrs[0])\n",
        "    return model\n",
        "\n",
        "\n",
        "def set_drophead(model, p=0.1):\n",
        "    \"\"\"\n",
        "    Adds drophead to model. Works inplace.\n",
        "    Args:\n",
        "        model: an instance of transformers.BertModel / transformers.RobertaModel /\n",
        "            transformers.XLMRobertaModel or downstream model (e.g. transformers.BertForSequenceClassification)\n",
        "            or any custom downstream model\n",
        "        p: drophead probability\n",
        "    \"\"\"\n",
        "    if (p < 0) or (p > 1):\n",
        "        raise ValueError(\"Wrong p argument\")\n",
        "\n",
        "    model = get_base_model(model)\n",
        "\n",
        "    for bert_layer in model.encoder.layer:\n",
        "        if not hasattr(bert_layer.attention.self, \"p_drophead\"):\n",
        "            bert_layer.attention.self.register_forward_hook(_drophead_hook)\n",
        "        bert_layer.attention.self.p_drophead = p"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A2zjBZZSrbW"
      },
      "source": [
        "### <b>4. Fine-tune Roberta on our dataset</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_r5TsAIiLJ_p"
      },
      "source": [
        "train_examples = processor.get_train_examples(train_df) # create train examples collection"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79Kxfp7xLk77",
        "outputId": "c118d337-5298-477a-ded8-d05e470f3661"
      },
      "source": [
        "num_train_optimization_steps = int(len(train_examples) / train_batch_size / gradient_accumulation_steps) * num_train_epochs\n",
        "print(num_train_optimization_steps)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "186.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292,
          "referenced_widgets": [
            "2e9250de84d9499888d2fdd71c0689fc",
            "48e4c646b2744792a3671abdf3d4f87b",
            "b1dd654a858848339fd3645473130c55",
            "84764d9badb64b18a76fe90e1b345cc8",
            "bb0697eab67b4040a2eb5466e50b6a4c",
            "bd6087ff0d7c4106949e96b52fac484f",
            "178ea1a7b4244aedb19de094d647c581",
            "ef889e5c3d144ad09f5e12bc50dee5bc",
            "620c1a42ce7e44c4afef538f8e52d7c3",
            "8334f402ed204748928392aeaa0605d4",
            "ef1cb1a0789b46c383245497d6cddf54",
            "a58e4b2820e346fdb2f918d02d33692c",
            "ff6fb2dcbfcb4a399978f4cd464eaba5",
            "c4061bdbca4f4f66948ee625b186a32c",
            "f025cb9bee684fd88382d4aeff2b0227",
            "ea5f6ee1e21b4b2ba8d780bff052b309"
          ]
        },
        "id": "IKE_6rcNLvrG",
        "outputId": "128afe05-aa64-4b45-9073-925b6995a798"
      },
      "source": [
        "model = RobertaForSequenceClassification.from_pretrained(roberta_model, cache_dir=cache_dir, num_labels=num_labels) # download roberta-base model"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:filelock:Lock 139679389975832 acquired on pytorch_pretrained_models/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2e9250de84d9499888d2fdd71c0689fc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=481.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:filelock:Lock 139679389975832 released on pytorch_pretrained_models/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:filelock:Lock 139679387995216 acquired on pytorch_pretrained_models/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "620c1a42ce7e44c4afef538f8e52d7c3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=501200538.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:filelock:Lock 139679387995216 released on pytorch_pretrained_models/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVywJXMDtzvv"
      },
      "source": [
        "set_drophead(model, p=0.07)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyfuJ5PuLy5X",
        "outputId": "2c13b5f8-3a09-40d9-dde3-4df8a16d7f80"
      },
      "source": [
        "model.to(device) # set model on GPU"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLLa3FPVL3k5"
      },
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "# adam optimizer\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, correct_bias=False) \n",
        "\n",
        "# scheduler for tuning learning rate over training time\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_train_optimization_steps)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQTYkm9UL8UR"
      },
      "source": [
        "global_step = 0\n",
        "nb_tr_steps = 0\n",
        "tr_loss = 0"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAZpjpdrL-n9",
        "outputId": "cfd2793f-1b12-436f-d163-4b9cb5b85ff3"
      },
      "source": [
        "from transformers import glue_convert_examples_to_features as convert_examples_to_features\n",
        "\n",
        "# create training features that roberta model understands\n",
        "train_features = convert_examples_to_features(\n",
        "    train_examples,\n",
        "    tokenizer,\n",
        "    max_length=max_seq_length,\n",
        "    label_list=label_list,\n",
        "    output_mode='classification'\n",
        ")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/data/processors/glue.py:67: FutureWarning: This function will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING.format(\"function\"), FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpqONYctMC3q",
        "outputId": "002a9f75-8350-4a72-c4d1-0835ed3ed9da"
      },
      "source": [
        "print(\"***** Running training *****\")\n",
        "print(\"Num examples =\", len(train_examples))\n",
        "print(\"Batch size =\", train_batch_size)\n",
        "print(\"Num steps =\", num_train_optimization_steps)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "Num examples = 2000\n",
            "Batch size = 32\n",
            "Num steps = 186.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQ7Q376oMPyh"
      },
      "source": [
        "# create a training dataset for pytorch using dataloader\n",
        "\n",
        "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
        "all_attention_mask = torch.tensor([f.attention_mask for f in train_features], dtype=torch.long)\n",
        "all_labels = torch.tensor([f.label for f in train_features], dtype=torch.long)\n",
        "\n",
        "train_data = TensorDataset(all_input_ids, all_attention_mask, all_labels)\n",
        "\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qi41oZB1MTLk",
        "outputId": "da42cd99-957c-4634-8e0b-c616c01b6e67"
      },
      "source": [
        "# train model with batch size = 32 with binary crossentropy loss\n",
        "\n",
        "model.train()\n",
        "\n",
        "weights = torch.tensor([2, 1], dtype=torch.float, device=device)\n",
        "\n",
        "for _ in trange(int(num_train_epochs), desc='Epoch'):\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    \n",
        "    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[2]}\n",
        "        \n",
        "        input_ids = batch[0]\n",
        "        label_ids = batch[2]\n",
        "        \n",
        "        outputs = model(**inputs)\n",
        "        \n",
        "        logits = outputs[1]\n",
        "        \n",
        "        loss_fct = CrossEntropyLoss(weight=weights)\n",
        "        loss = loss_fct(logits.view(-1, num_labels), label_ids.view(-1))\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        \n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        global_step += 1\n",
        "        \n",
        "    print(\"Training Loss: %s\" % (str(tr_loss)))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Iteration:   0%|          | 0/63 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2%|▏         | 1/63 [00:01<01:29,  1.44s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 2/63 [00:02<01:24,  1.38s/it]\u001b[A\n",
            "Iteration:   5%|▍         | 3/63 [00:03<01:20,  1.34s/it]\u001b[A\n",
            "Iteration:   6%|▋         | 4/63 [00:05<01:17,  1.32s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 5/63 [00:06<01:15,  1.31s/it]\u001b[A\n",
            "Iteration:  10%|▉         | 6/63 [00:07<01:13,  1.30s/it]\u001b[A\n",
            "Iteration:  11%|█         | 7/63 [00:09<01:12,  1.29s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 8/63 [00:10<01:10,  1.29s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 9/63 [00:11<01:09,  1.29s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 10/63 [00:12<01:08,  1.29s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 11/63 [00:14<01:07,  1.29s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 12/63 [00:15<01:05,  1.29s/it]\u001b[A\n",
            "Iteration:  21%|██        | 13/63 [00:16<01:04,  1.29s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 14/63 [00:18<01:03,  1.29s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 15/63 [00:19<01:01,  1.29s/it]\u001b[A\n",
            "Iteration:  25%|██▌       | 16/63 [00:20<01:00,  1.29s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 17/63 [00:21<00:59,  1.29s/it]\u001b[A\n",
            "Iteration:  29%|██▊       | 18/63 [00:23<00:58,  1.30s/it]\u001b[A\n",
            "Iteration:  30%|███       | 19/63 [00:24<00:57,  1.30s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 20/63 [00:25<00:55,  1.30s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 21/63 [00:27<00:54,  1.30s/it]\u001b[A\n",
            "Iteration:  35%|███▍      | 22/63 [00:28<00:53,  1.30s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 23/63 [00:29<00:52,  1.31s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 24/63 [00:31<00:51,  1.31s/it]\u001b[A\n",
            "Iteration:  40%|███▉      | 25/63 [00:32<00:49,  1.31s/it]\u001b[A\n",
            "Iteration:  41%|████▏     | 26/63 [00:33<00:48,  1.31s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 27/63 [00:35<00:47,  1.31s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 28/63 [00:36<00:46,  1.32s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 29/63 [00:37<00:44,  1.32s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 30/63 [00:39<00:43,  1.32s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 31/63 [00:40<00:42,  1.32s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 32/63 [00:41<00:41,  1.33s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 33/63 [00:43<00:39,  1.33s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 34/63 [00:44<00:38,  1.33s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 35/63 [00:45<00:37,  1.34s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 36/63 [00:47<00:36,  1.34s/it]\u001b[A\n",
            "Iteration:  59%|█████▊    | 37/63 [00:48<00:35,  1.35s/it]\u001b[A\n",
            "Iteration:  60%|██████    | 38/63 [00:49<00:33,  1.35s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 39/63 [00:51<00:32,  1.35s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 40/63 [00:52<00:31,  1.36s/it]\u001b[A\n",
            "Iteration:  65%|██████▌   | 41/63 [00:53<00:29,  1.36s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 42/63 [00:55<00:28,  1.36s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 43/63 [00:56<00:27,  1.37s/it]\u001b[A\n",
            "Iteration:  70%|██████▉   | 44/63 [00:58<00:26,  1.37s/it]\u001b[A\n",
            "Iteration:  71%|███████▏  | 45/63 [00:59<00:24,  1.38s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 46/63 [01:00<00:23,  1.38s/it]\u001b[A\n",
            "Iteration:  75%|███████▍  | 47/63 [01:02<00:22,  1.38s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 48/63 [01:03<00:20,  1.38s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 49/63 [01:04<00:19,  1.38s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 50/63 [01:06<00:18,  1.39s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 51/63 [01:07<00:16,  1.39s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 52/63 [01:09<00:15,  1.39s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 53/63 [01:10<00:13,  1.40s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 54/63 [01:11<00:12,  1.40s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 55/63 [01:13<00:11,  1.40s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 56/63 [01:14<00:09,  1.41s/it]\u001b[A\n",
            "Iteration:  90%|█████████ | 57/63 [01:16<00:08,  1.41s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 58/63 [01:17<00:07,  1.41s/it]\u001b[A\n",
            "Iteration:  94%|█████████▎| 59/63 [01:19<00:05,  1.42s/it]\u001b[A\n",
            "Iteration:  95%|█████████▌| 60/63 [01:20<00:04,  1.42s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 61/63 [01:21<00:02,  1.43s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 62/63 [01:23<00:01,  1.43s/it]\u001b[A\n",
            "Iteration: 100%|██████████| 63/63 [01:24<00:00,  1.34s/it]\n",
            "Epoch:  33%|███▎      | 1/3 [01:24<02:48, 84.11s/it]\n",
            "Iteration:   0%|          | 0/63 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss: 38.55598162859678\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:   2%|▏         | 1/63 [00:01<01:29,  1.44s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 2/63 [00:02<01:28,  1.44s/it]\u001b[A\n",
            "Iteration:   5%|▍         | 3/63 [00:04<01:26,  1.45s/it]\u001b[A\n",
            "Iteration:   6%|▋         | 4/63 [00:05<01:25,  1.45s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 5/63 [00:07<01:24,  1.46s/it]\u001b[A\n",
            "Iteration:  10%|▉         | 6/63 [00:08<01:23,  1.46s/it]\u001b[A\n",
            "Iteration:  11%|█         | 7/63 [00:10<01:21,  1.46s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 8/63 [00:11<01:20,  1.47s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 9/63 [00:13<01:19,  1.47s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 10/63 [00:14<01:17,  1.47s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 11/63 [00:16<01:16,  1.47s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 12/63 [00:17<01:15,  1.48s/it]\u001b[A\n",
            "Iteration:  21%|██        | 13/63 [00:19<01:13,  1.48s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 14/63 [00:20<01:12,  1.48s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 15/63 [00:22<01:10,  1.48s/it]\u001b[A\n",
            "Iteration:  25%|██▌       | 16/63 [00:23<01:09,  1.47s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 17/63 [00:24<01:07,  1.47s/it]\u001b[A\n",
            "Iteration:  29%|██▊       | 18/63 [00:26<01:06,  1.47s/it]\u001b[A\n",
            "Iteration:  30%|███       | 19/63 [00:27<01:04,  1.47s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 20/63 [00:29<01:03,  1.47s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 21/63 [00:30<01:01,  1.46s/it]\u001b[A\n",
            "Iteration:  35%|███▍      | 22/63 [00:32<00:59,  1.46s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 23/63 [00:33<00:58,  1.45s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 24/63 [00:35<00:56,  1.45s/it]\u001b[A\n",
            "Iteration:  40%|███▉      | 25/63 [00:36<00:54,  1.44s/it]\u001b[A\n",
            "Iteration:  41%|████▏     | 26/63 [00:38<00:53,  1.44s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 27/63 [00:39<00:51,  1.44s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 28/63 [00:40<00:50,  1.44s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 29/63 [00:42<00:48,  1.43s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 30/63 [00:43<00:47,  1.43s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 31/63 [00:45<00:45,  1.43s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 32/63 [00:46<00:44,  1.43s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 33/63 [00:47<00:42,  1.43s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 34/63 [00:49<00:41,  1.42s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 35/63 [00:50<00:39,  1.42s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 36/63 [00:52<00:38,  1.42s/it]\u001b[A\n",
            "Iteration:  59%|█████▊    | 37/63 [00:53<00:36,  1.42s/it]\u001b[A\n",
            "Iteration:  60%|██████    | 38/63 [00:55<00:35,  1.42s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 39/63 [00:56<00:34,  1.42s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 40/63 [00:57<00:32,  1.42s/it]\u001b[A\n",
            "Iteration:  65%|██████▌   | 41/63 [00:59<00:31,  1.42s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 42/63 [01:00<00:29,  1.42s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 43/63 [01:02<00:28,  1.42s/it]\u001b[A\n",
            "Iteration:  70%|██████▉   | 44/63 [01:03<00:27,  1.42s/it]\u001b[A\n",
            "Iteration:  71%|███████▏  | 45/63 [01:05<00:25,  1.42s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 46/63 [01:06<00:24,  1.42s/it]\u001b[A\n",
            "Iteration:  75%|███████▍  | 47/63 [01:07<00:22,  1.42s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 48/63 [01:09<00:21,  1.43s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 49/63 [01:10<00:20,  1.43s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 50/63 [01:12<00:18,  1.43s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 51/63 [01:13<00:17,  1.43s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 52/63 [01:15<00:15,  1.44s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 53/63 [01:16<00:14,  1.44s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 54/63 [01:17<00:12,  1.44s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 55/63 [01:19<00:11,  1.44s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 56/63 [01:20<00:10,  1.44s/it]\u001b[A\n",
            "Iteration:  90%|█████████ | 57/63 [01:22<00:08,  1.44s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 58/63 [01:23<00:07,  1.45s/it]\u001b[A\n",
            "Iteration:  94%|█████████▎| 59/63 [01:25<00:05,  1.45s/it]\u001b[A\n",
            "Iteration:  95%|█████████▌| 60/63 [01:26<00:04,  1.45s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 61/63 [01:28<00:02,  1.45s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 62/63 [01:29<00:01,  1.45s/it]\u001b[A\n",
            "Iteration: 100%|██████████| 63/63 [01:30<00:00,  1.43s/it]\n",
            "Epoch:  67%|██████▋   | 2/3 [02:54<01:25, 85.97s/it]\n",
            "Iteration:   0%|          | 0/63 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss: 20.06460191309452\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:   2%|▏         | 1/63 [00:01<01:30,  1.46s/it]\u001b[A\n",
            "Iteration:   3%|▎         | 2/63 [00:02<01:28,  1.46s/it]\u001b[A\n",
            "Iteration:   5%|▍         | 3/63 [00:04<01:27,  1.46s/it]\u001b[A\n",
            "Iteration:   6%|▋         | 4/63 [00:05<01:26,  1.46s/it]\u001b[A\n",
            "Iteration:   8%|▊         | 5/63 [00:07<01:24,  1.46s/it]\u001b[A\n",
            "Iteration:  10%|▉         | 6/63 [00:08<01:23,  1.46s/it]\u001b[A\n",
            "Iteration:  11%|█         | 7/63 [00:10<01:21,  1.46s/it]\u001b[A\n",
            "Iteration:  13%|█▎        | 8/63 [00:11<01:20,  1.46s/it]\u001b[A\n",
            "Iteration:  14%|█▍        | 9/63 [00:13<01:18,  1.46s/it]\u001b[A\n",
            "Iteration:  16%|█▌        | 10/63 [00:14<01:17,  1.45s/it]\u001b[A\n",
            "Iteration:  17%|█▋        | 11/63 [00:16<01:15,  1.45s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 12/63 [00:17<01:14,  1.45s/it]\u001b[A\n",
            "Iteration:  21%|██        | 13/63 [00:18<01:12,  1.45s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 14/63 [00:20<01:11,  1.46s/it]\u001b[A\n",
            "Iteration:  24%|██▍       | 15/63 [00:21<01:09,  1.45s/it]\u001b[A\n",
            "Iteration:  25%|██▌       | 16/63 [00:23<01:08,  1.45s/it]\u001b[A\n",
            "Iteration:  27%|██▋       | 17/63 [00:24<01:06,  1.45s/it]\u001b[A\n",
            "Iteration:  29%|██▊       | 18/63 [00:26<01:05,  1.45s/it]\u001b[A\n",
            "Iteration:  30%|███       | 19/63 [00:27<01:03,  1.45s/it]\u001b[A\n",
            "Iteration:  32%|███▏      | 20/63 [00:29<01:02,  1.45s/it]\u001b[A\n",
            "Iteration:  33%|███▎      | 21/63 [00:30<01:00,  1.45s/it]\u001b[A\n",
            "Iteration:  35%|███▍      | 22/63 [00:31<00:59,  1.45s/it]\u001b[A\n",
            "Iteration:  37%|███▋      | 23/63 [00:33<00:57,  1.45s/it]\u001b[A\n",
            "Iteration:  38%|███▊      | 24/63 [00:34<00:56,  1.44s/it]\u001b[A\n",
            "Iteration:  40%|███▉      | 25/63 [00:36<00:54,  1.44s/it]\u001b[A\n",
            "Iteration:  41%|████▏     | 26/63 [00:37<00:53,  1.44s/it]\u001b[A\n",
            "Iteration:  43%|████▎     | 27/63 [00:39<00:51,  1.44s/it]\u001b[A\n",
            "Iteration:  44%|████▍     | 28/63 [00:40<00:50,  1.44s/it]\u001b[A\n",
            "Iteration:  46%|████▌     | 29/63 [00:42<00:48,  1.44s/it]\u001b[A\n",
            "Iteration:  48%|████▊     | 30/63 [00:43<00:47,  1.44s/it]\u001b[A\n",
            "Iteration:  49%|████▉     | 31/63 [00:44<00:46,  1.44s/it]\u001b[A\n",
            "Iteration:  51%|█████     | 32/63 [00:46<00:44,  1.44s/it]\u001b[A\n",
            "Iteration:  52%|█████▏    | 33/63 [00:47<00:43,  1.44s/it]\u001b[A\n",
            "Iteration:  54%|█████▍    | 34/63 [00:49<00:41,  1.44s/it]\u001b[A\n",
            "Iteration:  56%|█████▌    | 35/63 [00:50<00:40,  1.44s/it]\u001b[A\n",
            "Iteration:  57%|█████▋    | 36/63 [00:52<00:38,  1.44s/it]\u001b[A\n",
            "Iteration:  59%|█████▊    | 37/63 [00:53<00:37,  1.44s/it]\u001b[A\n",
            "Iteration:  60%|██████    | 38/63 [00:54<00:35,  1.44s/it]\u001b[A\n",
            "Iteration:  62%|██████▏   | 39/63 [00:56<00:34,  1.44s/it]\u001b[A\n",
            "Iteration:  63%|██████▎   | 40/63 [00:57<00:33,  1.44s/it]\u001b[A\n",
            "Iteration:  65%|██████▌   | 41/63 [00:59<00:31,  1.44s/it]\u001b[A\n",
            "Iteration:  67%|██████▋   | 42/63 [01:00<00:30,  1.44s/it]\u001b[A\n",
            "Iteration:  68%|██████▊   | 43/63 [01:02<00:28,  1.44s/it]\u001b[A\n",
            "Iteration:  70%|██████▉   | 44/63 [01:03<00:27,  1.44s/it]\u001b[A\n",
            "Iteration:  71%|███████▏  | 45/63 [01:05<00:25,  1.44s/it]\u001b[A\n",
            "Iteration:  73%|███████▎  | 46/63 [01:06<00:24,  1.44s/it]\u001b[A\n",
            "Iteration:  75%|███████▍  | 47/63 [01:07<00:23,  1.44s/it]\u001b[A\n",
            "Iteration:  76%|███████▌  | 48/63 [01:09<00:21,  1.44s/it]\u001b[A\n",
            "Iteration:  78%|███████▊  | 49/63 [01:10<00:20,  1.44s/it]\u001b[A\n",
            "Iteration:  79%|███████▉  | 50/63 [01:12<00:18,  1.44s/it]\u001b[A\n",
            "Iteration:  81%|████████  | 51/63 [01:13<00:17,  1.44s/it]\u001b[A\n",
            "Iteration:  83%|████████▎ | 52/63 [01:15<00:15,  1.44s/it]\u001b[A\n",
            "Iteration:  84%|████████▍ | 53/63 [01:16<00:14,  1.44s/it]\u001b[A\n",
            "Iteration:  86%|████████▌ | 54/63 [01:18<00:12,  1.44s/it]\u001b[A\n",
            "Iteration:  87%|████████▋ | 55/63 [01:19<00:11,  1.44s/it]\u001b[A\n",
            "Iteration:  89%|████████▉ | 56/63 [01:20<00:10,  1.44s/it]\u001b[A\n",
            "Iteration:  90%|█████████ | 57/63 [01:22<00:08,  1.44s/it]\u001b[A\n",
            "Iteration:  92%|█████████▏| 58/63 [01:23<00:07,  1.44s/it]\u001b[A\n",
            "Iteration:  94%|█████████▎| 59/63 [01:25<00:05,  1.44s/it]\u001b[A\n",
            "Iteration:  95%|█████████▌| 60/63 [01:26<00:04,  1.45s/it]\u001b[A\n",
            "Iteration:  97%|█████████▋| 61/63 [01:28<00:02,  1.44s/it]\u001b[A\n",
            "Iteration:  98%|█████████▊| 62/63 [01:29<00:01,  1.44s/it]\u001b[A\n",
            "Iteration: 100%|██████████| 63/63 [01:30<00:00,  1.43s/it]\n",
            "Epoch: 100%|██████████| 3/3 [04:24<00:00, 88.26s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Loss: 11.961962923407555\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGrgihrAea_v"
      },
      "source": [
        "Save finetuned model to output directory, then use it for training on our dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQHM36SPQqBU",
        "outputId": "2fe8755a-74ee-4769-ea47-f40d23be2b9f"
      },
      "source": [
        "model_to_save = model.module if hasattr(model, 'module') else model \n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "model = RobertaForSequenceClassification.from_pretrained(output_dir)\n",
        "tokenizer = RobertaTokenizer.from_pretrained(output_dir)\n",
        "\n",
        "model.to(device)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCKnRPdlR6MJ"
      },
      "source": [
        "### <b>5. Evaluate model on Test dataset</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "abUwoZJYMlXA",
        "outputId": "31bc39e5-8a59-477c-daea-4e557b7c1b73"
      },
      "source": [
        "test_df = pd.read_csv('/content/gdrive/My Drive/products_sentiment_test.tsv', sep='\\t')\n",
        "\n",
        "test_df['label'] = 0\n",
        "test_df['label'] = test_df['label'].astype(str)\n",
        "test_df['text'] = test_df['text'].apply(lambda x: preprocess(x)).astype(str)\n",
        "\n",
        "test_df.head()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>so  why the small digital elph  rather than on...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>way through the first disk we played on it  n...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>better for the zen micro is outlook compatibil...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>play gameboy color games on it with goboy</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>likewise  i ve heard norton  professional vers...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id                                               text label\n",
              "0   0  so  why the small digital elph  rather than on...     0\n",
              "1   1   way through the first disk we played on it  n...     0\n",
              "2   2  better for the zen micro is outlook compatibil...     0\n",
              "3   3         play gameboy color games on it with goboy      0\n",
              "4   4  likewise  i ve heard norton  professional vers...     0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opQ-KdiYNIt7",
        "outputId": "d6b940fe-d76e-490f-b2e4-9117418e66f7"
      },
      "source": [
        "test_examples = processor.get_dev_examples(test_df)\n",
        "\n",
        "test_features = convert_examples_to_features(\n",
        "    test_examples,\n",
        "    tokenizer,\n",
        "    max_length=max_seq_length,\n",
        "    label_list=label_list,\n",
        "    output_mode='classification'\n",
        ")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/data/processors/glue.py:67: FutureWarning: This function will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING.format(\"function\"), FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3AbTv0fNOhc"
      },
      "source": [
        "all_input_ids = torch.tensor([f.input_ids for f in test_features], dtype=torch.long)\n",
        "all_attention_mask = torch.tensor([f.attention_mask for f in test_features], dtype=torch.long)\n",
        "all_labels = torch.tensor([f.label for f in test_features], dtype=torch.long)\n",
        "\n",
        "test_batch_size = 8\n",
        "\n",
        "test_data = TensorDataset(all_input_ids, all_attention_mask, all_labels)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=test_batch_size)\n",
        "\n",
        "test_loss = 0\n",
        "nb_test_steps = 0\n",
        "preds = []"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waOTNNaQNUiK",
        "outputId": "00750078-0ebc-4f5a-d766-df0735f42d0a"
      },
      "source": [
        "model.eval()\n",
        "\n",
        "for batch in tqdm(test_dataloader, desc=\"Predicting\"):\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[2]}\n",
        "        \n",
        "    input_ids = batch[0]\n",
        "    label_ids = batch[2]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        \n",
        "    logits = outputs[1]\n",
        "\n",
        "    # create eval loss and other metric required by the task\n",
        "    loss_fct = CrossEntropyLoss()\n",
        "    \n",
        "    tmp_test_loss = loss_fct(logits.view(-1, num_labels), label_ids.view(-1))\n",
        "    test_loss += tmp_test_loss.mean().item()\n",
        "    \n",
        "    nb_test_steps += 1\n",
        "    \n",
        "    if len(preds) == 0:\n",
        "        preds.append(logits.detach().cpu().numpy())\n",
        "    else:\n",
        "        preds[0] = np.append(preds[0], logits.detach().cpu().numpy(), axis=0)\n",
        "\n",
        "test_loss = test_loss / nb_test_steps\n",
        "\n",
        "print('test loss', test_loss)\n",
        "\n",
        "preds = preds[0]\n",
        "preds = np.argmax(preds, axis=1)\n",
        "\n",
        "print(preds.shape)\n",
        "\n",
        "print(preds)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting: 100%|██████████| 63/63 [00:07<00:00,  8.25it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test loss 2.573207563824124\n",
            "(500,)\n",
            "[0 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 0 1 1 1\n",
            " 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1 0\n",
            " 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0\n",
            " 0 1 0 1 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 0 0\n",
            " 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1\n",
            " 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1 1 0 1 1\n",
            " 1 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1\n",
            " 1 1 0 0 1 1 0 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0 1 0 1 1 0 1 1 1\n",
            " 1 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 1 0\n",
            " 1 1 1 1 1 1 0 1 0 0 0 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 0\n",
            " 1 1 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0\n",
            " 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 0 0 0 1 1\n",
            " 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1\n",
            " 1 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_yPneSknI4e"
      },
      "source": [
        " ### <b>6. Create Submission</b>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hik1bWjzew1Q"
      },
      "source": [
        "Make a CSV submission file for Kaggle using model predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "RJUCRdReOFn1",
        "outputId": "1a0ff02d-674c-4c6b-a7fe-553f8c0d03af"
      },
      "source": [
        "submission = test_df.copy()\n",
        "submission.drop('text', axis=1, inplace=True)\n",
        "submission.label = preds\n",
        "submission.columns = ['Id', 'y']\n",
        "submission"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>495</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>496</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>497</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>498</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>499</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Id  y\n",
              "0      0  0\n",
              "1      1  0\n",
              "2      2  1\n",
              "3      3  1\n",
              "4      4  1\n",
              "..   ... ..\n",
              "495  495  0\n",
              "496  496  1\n",
              "497  497  0\n",
              "498  498  1\n",
              "499  499  1\n",
              "\n",
              "[500 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEQxqn9ye4EP"
      },
      "source": [
        "Save submission to CSV in order to load at Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxXL0cNrOWqm"
      },
      "source": [
        "submission.to_csv('submission.csv', index=None)"
      ],
      "execution_count": 33,
      "outputs": []
    }
  ]
}